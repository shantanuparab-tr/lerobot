usage: train.py [-h] [--config_path str] [--dataset str]
                [--dataset.repo_id str] [--dataset.episodes [List]]
                [--image_transforms str]
                [--dataset.image_transforms.enable bool]
                [--dataset.image_transforms.max_num_transforms int]
                [--dataset.image_transforms.random_order bool]
                [--dataset.image_transforms.tfs Dict]
                [--dataset.local_files_only bool]
                [--dataset.use_imagenet_stats bool]
                [--dataset.video_backend str] [--env str]
                [--env.type {aloha,pusht,xarm}] [--env.task str]
                [--env.fps int] [--env.features Dict]
                [--env.features_map Dict] [--env.episode_length int]
                [--env.obs_type str] [--env.render_mode str]
                [--env.visualization_width int]
                [--env.visualization_height int] [--policy str]
                [--policy.type {act,diffusion,pi0,tdmpc,vqbet}]
                [--policy.replace_final_stride_with_dilation int]
                [--policy.pre_norm bool] [--policy.dim_model int]
                [--policy.n_heads int] [--policy.dim_feedforward int]
                [--policy.feedforward_activation str]
                [--policy.n_encoder_layers int]
                [--policy.n_decoder_layers int] [--policy.use_vae bool]
                [--policy.n_vae_encoder_layers int]
                [--policy.temporal_ensemble_coeff [float]]
                [--policy.kl_weight float]
                [--policy.optimizer_lr_backbone float]
                [--policy.drop_n_last_frames int]
                [--policy.use_separate_rgb_encoder_per_camera bool]
                [--policy.down_dims int [int, ...]] [--policy.kernel_size int]
                [--policy.n_groups int]
                [--policy.diffusion_step_embed_dim int]
                [--policy.use_film_scale_modulation bool]
                [--policy.noise_scheduler_type str]
                [--policy.num_train_timesteps int]
                [--policy.beta_schedule str] [--policy.beta_start float]
                [--policy.beta_end float] [--policy.prediction_type str]
                [--policy.clip_sample bool] [--policy.clip_sample_range float]
                [--policy.num_inference_steps [int]]
                [--policy.do_mask_loss_for_padding bool]
                [--policy.scheduler_name str] [--policy.chunk_size int]
                [--policy.max_state_dim int] [--policy.max_action_dim int]
                [--policy.resize_imgs_with_padding int int]
                [--policy.empty_cameras int] [--policy.adapt_to_pi_aloha bool]
                [--policy.use_delta_joint_actions_aloha bool]
                [--policy.tokenizer_max_length int] [--policy.proj_width int]
                [--policy.num_steps int] [--policy.use_cache bool]
                [--policy.attention_implementation str]
                [--policy.freeze_vision_encoder bool]
                [--policy.train_expert_only bool]
                [--policy.train_state_proj bool]
                [--policy.scheduler_decay_steps int]
                [--policy.scheduler_decay_lr float]
                [--policy.n_action_repeats int] [--policy.horizon int]
                [--policy.n_action_steps int]
                [--policy.image_encoder_hidden_dim int]
                [--policy.state_encoder_hidden_dim int]
                [--policy.latent_dim int] [--policy.q_ensemble_size int]
                [--policy.mlp_dim int] [--policy.discount float]
                [--policy.use_mpc bool] [--policy.cem_iterations int]
                [--policy.max_std float] [--policy.min_std float]
                [--policy.n_gaussian_samples int] [--policy.n_pi_samples int]
                [--policy.uncertainty_regularizer_coeff float]
                [--policy.n_elites int]
                [--policy.elite_weighting_temperature float]
                [--policy.gaussian_mean_momentum float]
                [--policy.max_random_shift_ratio float]
                [--policy.reward_coeff float]
                [--policy.expectile_weight float] [--policy.value_coeff float]
                [--policy.consistency_coeff float]
                [--policy.advantage_scaling float] [--policy.pi_coeff float]
                [--policy.temporal_decay_coeff float]
                [--policy.target_model_momentum float]
                [--policy.n_obs_steps int]
                [--policy.normalization_mapping Dict]
                [--policy.input_features Dict] [--policy.output_features Dict]
                [--policy.n_action_pred_token int]
                [--policy.action_chunk_size int]
                [--policy.vision_backbone str] [--policy.crop_shape [int int]]
                [--policy.crop_is_random bool]
                [--policy.pretrained_backbone_weights [str]]
                [--policy.use_group_norm bool]
                [--policy.spatial_softmax_num_keypoints int]
                [--policy.n_vqvae_training_steps int]
                [--policy.vqvae_n_embed int]
                [--policy.vqvae_embedding_dim int]
                [--policy.vqvae_enc_hidden_dim int]
                [--policy.gpt_block_size int] [--policy.gpt_input_dim int]
                [--policy.gpt_output_dim int] [--policy.gpt_n_layer int]
                [--policy.gpt_n_head int] [--policy.gpt_hidden_dim int]
                [--policy.dropout float] [--policy.mlp_hidden_dim int]
                [--policy.offset_loss_weight float]
                [--policy.primary_code_loss_weight float]
                [--policy.secondary_code_loss_weight float]
                [--policy.bet_softmax_temperature float]
                [--policy.sequentially_select bool]
                [--policy.optimizer_lr float] [--policy.optimizer_betas Any]
                [--policy.optimizer_eps float]
                [--policy.optimizer_weight_decay float]
                [--policy.optimizer_vqvae_lr float]
                [--policy.optimizer_vqvae_weight_decay float]
                [--policy.scheduler_warmup_steps int] [--output_dir [Path]]
                [--job_name [str]] [--resume bool] [--device [str]]
                [--use_amp bool] [--seed [int]] [--num_workers int]
                [--batch_size int] [--eval_freq int] [--log_freq int]
                [--save_checkpoint bool] [--save_freq int] [--offline str]
                [--offline.steps int] [--online str] [--online.steps int]
                [--online.rollout_n_episodes int]
                [--online.rollout_batch_size int]
                [--online.steps_between_rollouts [int]]
                [--online.sampling_ratio float] [--online.env_seed [int]]
                [--online.buffer_capacity [int]]
                [--online.buffer_seed_size int]
                [--online.do_rollout_async bool]
                [--use_policy_training_preset bool] [--optimizer str]
                [--optimizer.type {adam,adamw,sgd}]
                [--optimizer.betas float float] [--optimizer.eps float]
                [--optimizer.lr float] [--optimizer.weight_decay float]
                [--optimizer.grad_clip_norm float]
                [--optimizer.momentum float] [--optimizer.dampening float]
                [--optimizer.nesterov bool] [--scheduler str]
                [--scheduler.type {diffuser,vqbet,cosine_decay_with_warmup}]
                [--scheduler.name str]
                [--scheduler.num_vqvae_training_steps int]
                [--scheduler.num_cycles float]
                [--scheduler.num_warmup_steps int]
                [--scheduler.num_decay_steps int] [--scheduler.peak_lr float]
                [--scheduler.decay_lr float] [--eval str]
                [--eval.n_episodes int] [--eval.batch_size int]
                [--eval.use_async_envs bool] [--wandb str]
                [--wandb.enable bool] [--wandb.disable_artifact bool]
                [--wandb.project str] [--wandb.entity [str]]
                [--wandb.notes [str]]

options:
  -h, --help            show this help message and exit
  --config_path str     Path for a config file to parse with draccus (default:
                        None)
  --dataset str         Config file for dataset (default: None)
  --image_transforms str
                        Config file for image_transforms (default: None)
  --env str             Config file for env (default: None)
  --policy str          Config file for policy (default: None)
  --offline str         Config file for offline (default: None)
  --online str          Config file for online (default: None)
  --optimizer str       Config file for optimizer (default: None)
  --scheduler str       Config file for scheduler (default: None)
  --eval str            Config file for eval (default: None)
  --wandb str           Config file for wandb (default: None)

TrainPipelineConfig:

  --output_dir [Path]   Set `dir` to where you would like to save all of the
                        run outputs. If you run another training session with
                        the same value for `dir` its contents will be
                        overwritten unless you set `resume` to true. (default:
                        None)
  --job_name [str]
  --resume bool         Set `resume` to true to resume a previous run. In
                        order for this to work, you will need to make sure
                        `dir` is the directory of an existing run with at
                        least one checkpoint in it. Note that when resuming a
                        run, the default behavior is to use the configuration
                        from the checkpoint, regardless of what's provided
                        with the training command at the time of resumption.
                        (default: False)
  --device [str]        cuda | cpu | mp (default: None)
  --use_amp bool        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --seed [int]          `seed` is used for training (eg: model initialization,
                        dataset shuffling) AND for the evaluation
                        environments. (default: 1000)
  --num_workers int     Number of workers for the dataloader. (default: 4)
  --batch_size int      
  --eval_freq int       
  --log_freq int        
  --save_checkpoint bool
  --save_freq int       Checkpoint is saved every `save_freq` training
                        iterations and after the last training step. (default:
                        20000)
  --use_policy_training_preset bool

DatasetConfig ['dataset']:

  --dataset.repo_id str
                        You may provide a list of datasets here. `train.py`
                        creates them all and concatenates them. Note: only
                        data keys common between the datasets are kept. Each
                        dataset gets and additional transform that inserts the
                        "dataset_index" into the returned item. The index
                        mapping is made according to the order in which the
                        datsets are provided. (default: None)
  --dataset.episodes [List]
  --dataset.local_files_only bool
  --dataset.use_imagenet_stats bool
  --dataset.video_backend str

ImageTransformsConfig ['dataset.image_transforms']:
  
      These transforms are all using standard torchvision.transforms.v2
      You can find out how these transformations affect images here:
      https://pytorch.org/vision/0.18/auto_examples/transforms/plot_transforms_illustrations.html
      We use a custom RandomSubsetApply container to sample them.
      

  --dataset.image_transforms.enable bool
                        Set this flag to `true` to enable transforms during
                        training (default: False)
  --dataset.image_transforms.max_num_transforms int
                        This is the maximum number of transforms (sampled from
                        these below) that will be applied to each frame. It's
                        an integer in the interval [1,
                        number_of_available_transforms]. (default: 3)
  --dataset.image_transforms.random_order bool
                        By default, transforms are applied in Torchvision's
                        suggested order (shown below). Set this to True to
                        apply them in a random order. (default: False)
  --dataset.image_transforms.tfs Dict

Optional ['env']:

EnvConfig ['env']:

  --env.type {aloha,pusht,xarm}
                        Which type of EnvConfig ['env'] to use (default: None)

AlohaEnv ['env']:

  --env.task str        
  --env.fps int         
  --env.features Dict   
  --env.features_map Dict
  --env.episode_length int
  --env.obs_type str    
  --env.render_mode str

PushtEnv ['env']:

  --env.task str        
  --env.fps int         
  --env.features Dict   
  --env.features_map Dict
  --env.episode_length int
  --env.obs_type str    
  --env.render_mode str
  --env.visualization_width int
  --env.visualization_height int

XarmEnv ['env']:

  --env.task str        
  --env.fps int         
  --env.features Dict   
  --env.features_map Dict
  --env.episode_length int
  --env.obs_type str    
  --env.render_mode str
  --env.visualization_width int
  --env.visualization_height int

Optional ['policy']:

PreTrainedConfig ['policy']:
  
      Base configuration class for policy models.
  
      Args:
          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the
              current step and additional steps going back).
          input_shapes: A dictionary defining the shapes of the input data for the policy.
          output_shapes: A dictionary defining the shapes of the output data for the policy.
          input_normalization_modes: A dictionary with key representing the modality and the value specifies the
              normalization mode to apply.
          output_normalization_modes: Similar dictionary as `input_normalization_modes`, but to unnormalize to
              the original scale.
      

  --policy.type {act,diffusion,pi0,tdmpc,vqbet}
                        Which type of PreTrainedConfig ['policy'] to use
                        (default: None)

ACTConfig ['policy']:
  Configuration class for the Action Chunking Transformers policy.
  
      Defaults are configured for training on bimanual Aloha tasks like "insertion" or "transfer".
  
      The parameters you will most likely need to change are the ones which depend on the environment / sensors.
      Those are: `input_shapes` and 'output_shapes`.
  
      Notes on the inputs and outputs:
          - Either:
              - At least one key starting with "observation.image is required as an input.
                AND/OR
              - The key "observation.environment_state" is required as input.
          - If there are multiple keys beginning with "observation.images." they are treated as multiple camera
            views. Right now we only support all images having the same shape.
          - May optionally work without an "observation.state" key for the proprioceptive robot state.
          - "action" is required as an output key.
  
      Args:
          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the
              current step and additional steps going back).
          chunk_size: The size of the action prediction "chunks" in units of environment steps.
          n_action_steps: The number of action steps to run in the environment for one invocation of the policy.
              This should be no greater than the chunk size. For example, if the chunk size size 100, you may
              set this to 50. This would mean that the model predicts 100 steps worth of actions, runs 50 in the
              environment, and throws the other 50 out.
          input_shapes: A dictionary defining the shapes of the input data for the policy. The key represents
              the input data name, and the value is a list indicating the dimensions of the corresponding data.
              For example, "observation.image" refers to an input from a camera with dimensions [3, 96, 96],
              indicating it has three color channels and 96x96 resolution. Importantly, `input_shapes` doesn't
              include batch dimension or temporal dimension.
          output_shapes: A dictionary defining the shapes of the output data for the policy. The key represents
              the output data name, and the value is a list indicating the dimensions of the corresponding data.
              For example, "action" refers to an output shape of [14], indicating 14-dimensional actions.
              Importantly, `output_shapes` doesn't include batch dimension or temporal dimension.
          input_normalization_modes: A dictionary with key representing the modality (e.g. "observation.state"),
              and the value specifies the normalization mode to apply. The two available modes are "mean_std"
              which subtracts the mean and divides by the standard deviation and "min_max" which rescale in a
              [-1, 1] range.
          output_normalization_modes: Similar dictionary as `normalize_input_modes`, but to unnormalize to the
              original scale. Note that this is also used for normalizing the training targets.
          vision_backbone: Name of the torchvision resnet backbone to use for encoding images.
          pretrained_backbone_weights: Pretrained weights from torchvision to initalize the backbone.
              `None` means no pretrained weights.
          replace_final_stride_with_dilation: Whether to replace the ResNet's final 2x2 stride with a dilated
              convolution.
          pre_norm: Whether to use "pre-norm" in the transformer blocks.
          dim_model: The transformer blocks' main hidden dimension.
          n_heads: The number of heads to use in the transformer blocks' multi-head attention.
          dim_feedforward: The dimension to expand the transformer's hidden dimension to in the feed-forward
              layers.
          feedforward_activation: The activation to use in the transformer block's feed-forward layers.
          n_encoder_layers: The number of transformer layers to use for the transformer encoder.
          n_decoder_layers: The number of transformer layers to use for the transformer decoder.
          use_vae: Whether to use a variational objective during training. This introduces another transformer
              which is used as the VAE's encoder (not to be confused with the transformer encoder - see
              documentation in the policy class).
          latent_dim: The VAE's latent dimension.
          n_vae_encoder_layers: The number of transformer layers to use for the VAE's encoder.
          temporal_ensemble_coeff: Coefficient for the exponential weighting scheme to apply for temporal
              ensembling. Defaults to None which means temporal ensembling is not used. `n_action_steps` must be
              1 when using this feature, as inference needs to happen at every step to form an ensemble. For
              more information on how ensembling works, please see `ACTTemporalEnsembler`.
          dropout: Dropout to use in the transformer layers (see code for details).
          kl_weight: The weight to use for the KL-divergence component of the loss if the variational objective
              is enabled. Loss is then calculated as: `reconstruction_loss + kl_weight * kld_loss`.
      

  --policy.n_obs_steps int
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.chunk_size int
  --policy.n_action_steps int
  --policy.vision_backbone str
  --policy.pretrained_backbone_weights [str]
  --policy.replace_final_stride_with_dilation int
  --policy.pre_norm bool
  --policy.dim_model int
  --policy.n_heads int  
  --policy.dim_feedforward int
  --policy.feedforward_activation str
  --policy.n_encoder_layers int
  --policy.n_decoder_layers int
  --policy.use_vae bool
  --policy.latent_dim int
  --policy.n_vae_encoder_layers int
  --policy.temporal_ensemble_coeff [float]
  --policy.dropout float
  --policy.kl_weight float
  --policy.optimizer_lr float
                        Training preset (default: 1e-05)
  --policy.optimizer_weight_decay float
  --policy.optimizer_lr_backbone float

DiffusionConfig ['policy']:
  Configuration class for DiffusionPolicy.
  
      Defaults are configured for training with PushT providing proprioceptive and single camera observations.
  
      The parameters you will most likely need to change are the ones which depend on the environment / sensors.
      Those are: `input_shapes` and `output_shapes`.
  
      Notes on the inputs and outputs:
          - "observation.state" is required as an input key.
          - Either:
              - At least one key starting with "observation.image is required as an input.
                AND/OR
              - The key "observation.environment_state" is required as input.
          - If there are multiple keys beginning with "observation.image" they are treated as multiple camera
            views. Right now we only support all images having the same shape.
          - "action" is required as an output key.
  
      Args:
          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the
              current step and additional steps going back).
          horizon: Diffusion model action prediction size as detailed in `DiffusionPolicy.select_action`.
          n_action_steps: The number of action steps to run in the environment for one invocation of the policy.
              See `DiffusionPolicy.select_action` for more details.
          input_shapes: A dictionary defining the shapes of the input data for the policy. The key represents
              the input data name, and the value is a list indicating the dimensions of the corresponding data.
              For example, "observation.image" refers to an input from a camera with dimensions [3, 96, 96],
              indicating it has three color channels and 96x96 resolution. Importantly, `input_shapes` doesn't
              include batch dimension or temporal dimension.
          output_shapes: A dictionary defining the shapes of the output data for the policy. The key represents
              the output data name, and the value is a list indicating the dimensions of the corresponding data.
              For example, "action" refers to an output shape of [14], indicating 14-dimensional actions.
              Importantly, `output_shapes` doesn't include batch dimension or temporal dimension.
          input_normalization_modes: A dictionary with key representing the modality (e.g. "observation.state"),
              and the value specifies the normalization mode to apply. The two available modes are "mean_std"
              which subtracts the mean and divides by the standard deviation and "min_max" which rescale in a
              [-1, 1] range.
          output_normalization_modes: Similar dictionary as `normalize_input_modes`, but to unnormalize to the
              original scale. Note that this is also used for normalizing the training targets.
          vision_backbone: Name of the torchvision resnet backbone to use for encoding images.
          crop_shape: (H, W) shape to crop images to as a preprocessing step for the vision backbone. Must fit
              within the image size. If None, no cropping is done.
          crop_is_random: Whether the crop should be random at training time (it's always a center crop in eval
              mode).
          pretrained_backbone_weights: Pretrained weights from torchvision to initalize the backbone.
              `None` means no pretrained weights.
          use_group_norm: Whether to replace batch normalization with group normalization in the backbone.
              The group sizes are set to be about 16 (to be precise, feature_dim // 16).
          spatial_softmax_num_keypoints: Number of keypoints for SpatialSoftmax.
          use_separate_rgb_encoders_per_camera: Whether to use a separate RGB encoder for each camera view.
          down_dims: Feature dimension for each stage of temporal downsampling in the diffusion modeling Unet.
              You may provide a variable number of dimensions, therefore also controlling the degree of
              downsampling.
          kernel_size: The convolutional kernel size of the diffusion modeling Unet.
          n_groups: Number of groups used in the group norm of the Unet's convolutional blocks.
          diffusion_step_embed_dim: The Unet is conditioned on the diffusion timestep via a small non-linear
              network. This is the output dimension of that network, i.e., the embedding dimension.
          use_film_scale_modulation: FiLM (https://arxiv.org/abs/1709.07871) is used for the Unet conditioning.
              Bias modulation is used be default, while this parameter indicates whether to also use scale
              modulation.
          noise_scheduler_type: Name of the noise scheduler to use. Supported options: ["DDPM", "DDIM"].
          num_train_timesteps: Number of diffusion steps for the forward diffusion schedule.
          beta_schedule: Name of the diffusion beta schedule as per DDPMScheduler from Hugging Face diffusers.
          beta_start: Beta value for the first forward-diffusion step.
          beta_end: Beta value for the last forward-diffusion step.
          prediction_type: The type of prediction that the diffusion modeling Unet makes. Choose from "epsilon"
              or "sample". These have equivalent outcomes from a latent variable modeling perspective, but
              "epsilon" has been shown to work better in many deep neural network settings.
          clip_sample: Whether to clip the sample to [-`clip_sample_range`, +`clip_sample_range`] for each
              denoising step at inference time. WARNING: you will need to make sure your action-space is
              normalized to fit within this range.
          clip_sample_range: The magnitude of the clipping range as described above.
          num_inference_steps: Number of reverse diffusion steps to use at inference time (steps are evenly
              spaced). If not provided, this defaults to be the same as `num_train_timesteps`.
          do_mask_loss_for_padding: Whether to mask the loss when there are copy-padded actions. See
              `LeRobotDataset` and `load_previous_and_future_frames` for mor information. Note, this defaults
              to False as the original Diffusion Policy implementation does the same.
      

  --policy.n_obs_steps int
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.horizon int  
  --policy.n_action_steps int
  --policy.drop_n_last_frames int
                        horizon - n_action_steps - n_obs_steps + 1 (default:
                        7)
  --policy.vision_backbone str
  --policy.crop_shape [int int]
  --policy.crop_is_random bool
  --policy.pretrained_backbone_weights [str]
  --policy.use_group_norm bool
  --policy.spatial_softmax_num_keypoints int
  --policy.use_separate_rgb_encoder_per_camera bool
  --policy.down_dims int [int, ...]
  --policy.kernel_size int
  --policy.n_groups int
  --policy.diffusion_step_embed_dim int
  --policy.use_film_scale_modulation bool
  --policy.noise_scheduler_type str
                        Noise scheduler. (default: DDPM)
  --policy.num_train_timesteps int
  --policy.beta_schedule str
  --policy.beta_start float
  --policy.beta_end float
  --policy.prediction_type str
  --policy.clip_sample bool
  --policy.clip_sample_range float
  --policy.num_inference_steps [int]
  --policy.do_mask_loss_for_padding bool
  --policy.optimizer_lr float
                        Training presets (default: 0.0001)
  --policy.optimizer_betas Any
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.scheduler_name str
  --policy.scheduler_warmup_steps int

PI0Config ['policy']:

  --policy.n_obs_steps int
                        Input / output structure. (default: 1)
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.chunk_size int
  --policy.n_action_steps int
  --policy.max_state_dim int
                        Shorter state and action vectors will be padded
                        (default: 32)
  --policy.max_action_dim int
  --policy.resize_imgs_with_padding int int
                        Image preprocessing (default: (224, 224))
  --policy.empty_cameras int
                        Add empty images. Used by pi0_aloha_sim which adds the
                        empty left and right wrist cameras in addition to the
                        top camera. (default: 0)
  --policy.adapt_to_pi_aloha bool
                        Converts the joint and gripper values from the
                        standard Aloha space to the space used by the pi
                        internal runtime which was used to train the base
                        model. (default: False)
  --policy.use_delta_joint_actions_aloha bool
                        Converts joint dimensions to deltas with respect to
                        the current state before passing to the model. Gripper
                        dimensions will remain in absolute values. (default:
                        False)
  --policy.tokenizer_max_length int
                        Tokenizer (default: 48)
  --policy.proj_width int
                        Projector (default: 1024)
  --policy.num_steps int
                        Decoding (default: 10)
  --policy.use_cache bool
                        Attention utils (default: True)
  --policy.attention_implementation str
                        or fa2, flex (default: eager)
  --policy.freeze_vision_encoder bool
                        Finetuning settings (default: True)
  --policy.train_expert_only bool
  --policy.train_state_proj bool
  --policy.optimizer_lr float
                        Training presets (default: 2.5e-05)
  --policy.optimizer_betas float float
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.scheduler_warmup_steps int
  --policy.scheduler_decay_steps int
  --policy.scheduler_decay_lr float

TDMPCConfig ['policy']:
  Configuration class for TDMPCPolicy.
  
      Defaults are configured for training with xarm_lift_medium_replay providing proprioceptive and single
      camera observations.
  
      The parameters you will most likely need to change are the ones which depend on the environment / sensors.
      Those are: `input_shapes`, `output_shapes`, and perhaps `max_random_shift_ratio`.
  
      Args:
          n_action_repeats: The number of times to repeat the action returned by the planning. (hint: Google
              action repeats in Q-learning or ask your favorite chatbot)
          horizon: Horizon for model predictive control.
          n_action_steps: Number of action steps to take from the plan given by model predictive control. This
              is an alternative to using action repeats. If this is set to more than 1, then we require
              `n_action_repeats == 1`, `use_mpc == True` and `n_action_steps <= horizon`. Note that this
              approach of using multiple steps from the plan is not in the original implementation.
          input_shapes: A dictionary defining the shapes of the input data for the policy. The key represents
              the input data name, and the value is a list indicating the dimensions of the corresponding data.
              For example, "observation.image" refers to an input from a camera with dimensions [3, 96, 96],
              indicating it has three color channels and 96x96 resolution. Importantly, `input_shapes` doesn't
              include batch dimension or temporal dimension.
          output_shapes: A dictionary defining the shapes of the output data for the policy. The key represents
              the output data name, and the value is a list indicating the dimensions of the corresponding data.
              For example, "action" refers to an output shape of [14], indicating 14-dimensional actions.
              Importantly, `output_shapes` doesn't include batch dimension or temporal dimension.
          input_normalization_modes: A dictionary with key representing the modality (e.g. "observation.state"),
              and the value specifies the normalization mode to apply. The two available modes are "mean_std"
              which subtracts the mean and divides by the standard deviation and "min_max" which rescale in a
              [-1, 1] range. Note that here this defaults to None meaning inputs are not normalized. This is to
              match the original implementation.
          output_normalization_modes: Similar dictionary as `normalize_input_modes`, but to unnormalize to the
              original scale. Note that this is also used for normalizing the training targets. NOTE: Clipping
              to [-1, +1] is used during MPPI/CEM. Therefore, it is recommended that you stick with "min_max"
              normalization mode here.
          image_encoder_hidden_dim: Number of channels for the convolutional layers used for image encoding.
          state_encoder_hidden_dim: Hidden dimension for MLP used for state vector encoding.
          latent_dim: Observation's latent embedding dimension.
          q_ensemble_size: Number of Q function estimators to use in an ensemble for uncertainty estimation.
          mlp_dim: Hidden dimension of MLPs used for modelling the dynamics encoder, reward function, policy
              (π), Q ensemble, and V.
          discount: Discount factor (γ) to use for the reinforcement learning formalism.
          use_mpc: Whether to use model predictive control. The alternative is to just sample the policy model
              (π) for each step.
          cem_iterations: Number of iterations for the MPPI/CEM loop in MPC.
          max_std: Maximum standard deviation for actions sampled from the gaussian PDF in CEM.
          min_std: Minimum standard deviation for noise applied to actions sampled from the policy model (π).
              Doubles up as the minimum standard deviation for actions sampled from the gaussian PDF in CEM.
          n_gaussian_samples: Number of samples to draw from the gaussian distribution every CEM iteration. Must
              be non-zero.
          n_pi_samples: Number of samples to draw from the policy / world model rollout every CEM iteration. Can
              be zero.
          uncertainty_regularizer_coeff: Coefficient for the uncertainty regularization used when estimating
              trajectory values (this is the λ coeffiecient in eqn 4 of FOWM).
          n_elites: The number of elite samples to use for updating the gaussian parameters every CEM iteration.
          elite_weighting_temperature: The temperature to use for softmax weighting (by trajectory value) of the
              elites, when updating the gaussian parameters for CEM.
          gaussian_mean_momentum: Momentum (α) used for EMA updates of the mean parameter μ of the gaussian
              parameters optimized in CEM. Updates are calculated as μ⁻ ← αμ⁻ + (1-α)μ.
          max_random_shift_ratio: Maximum random shift (as a proportion of the image size) to apply to the
              image(s) (in units of pixels) for training-time augmentation. If set to 0, no such augmentation
              is applied. Note that the input images are assumed to be square for this augmentation.
          reward_coeff: Loss weighting coefficient for the reward regression loss.
          expectile_weight: Weighting (τ) used in expectile regression for the state value function (V).
              v_pred < v_target is weighted by τ and v_pred >= v_target is weighted by (1-τ). τ is expected to
              be in [0, 1]. Setting τ closer to 1 results in a more "optimistic" V. This is sensible to do
              because v_target is obtained by evaluating the learned state-action value functions (Q) with
              in-sample actions that may not be always optimal.
          value_coeff: Loss weighting coefficient for both the state-action value (Q) TD loss, and the state
              value (V) expectile regression loss.
          consistency_coeff: Loss weighting coefficient for the consistency loss.
          advantage_scaling: A factor by which the advantages are scaled prior to exponentiation for advantage
              weighted regression of the policy (π) estimator parameters. Note that the exponentiated advantages
              are clamped at 100.0.
          pi_coeff: Loss weighting coefficient for the action regression loss.
          temporal_decay_coeff: Exponential decay coefficient for decaying the loss coefficient for future time-
              steps. Hint: each loss computation involves `horizon` steps worth of actions starting from the
              current time step.
          target_model_momentum: Momentum (α) used for EMA updates of the target models. Updates are calculated
              as ϕ ← αϕ + (1-α)θ where ϕ are the parameters of the target model and θ are the parameters of the
              model being trained.
      

  --policy.n_obs_steps int
                        Input / output structure. (default: 1)
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.n_action_repeats int
  --policy.horizon int  
  --policy.n_action_steps int
  --policy.image_encoder_hidden_dim int
  --policy.state_encoder_hidden_dim int
  --policy.latent_dim int
  --policy.q_ensemble_size int
  --policy.mlp_dim int  
  --policy.discount float
  --policy.use_mpc bool
  --policy.cem_iterations int
  --policy.max_std float
  --policy.min_std float
  --policy.n_gaussian_samples int
  --policy.n_pi_samples int
  --policy.uncertainty_regularizer_coeff float
  --policy.n_elites int
  --policy.elite_weighting_temperature float
  --policy.gaussian_mean_momentum float
  --policy.max_random_shift_ratio float
  --policy.reward_coeff float
  --policy.expectile_weight float
  --policy.value_coeff float
  --policy.consistency_coeff float
  --policy.advantage_scaling float
  --policy.pi_coeff float
  --policy.temporal_decay_coeff float
  --policy.target_model_momentum float
  --policy.optimizer_lr float
                        Training presets (default: 0.0003)

VQBeTConfig ['policy']:
  Configuration class for VQ-BeT.
  
      Defaults are configured for training with PushT providing proprioceptive and single camera observations.
  
      The parameters you will most likely need to change are the ones which depend on the environment / sensors.
      Those are: `input_shapes` and `output_shapes`.
  
      Notes on the inputs and outputs:
          - "observation.state" is required as an input key.
          - At least one key starting with "observation.image is required as an input.
          - If there are multiple keys beginning with "observation.image" they are treated as multiple camera
            views. Right now we only support all images having the same shape.
          - "action" is required as an output key.
  
      Args:
          n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the
              current step and additional steps going back).
          n_action_pred_token: Total number of current token and future tokens that VQ-BeT predicts.
          action_chunk_size: Action chunk size of each action prediction token.
          input_shapes: A dictionary defining the shapes of the input data for the policy.
              The key represents the input data name, and the value is a list indicating the dimensions
              of the corresponding data. For example, "observation.image" refers to an input from
              a camera with dimensions [3, 96, 96], indicating it has three color channels and 96x96 resolution.
              Importantly, shapes doesnt include batch dimension or temporal dimension.
          output_shapes: A dictionary defining the shapes of the output data for the policy.
              The key represents the output data name, and the value is a list indicating the dimensions
              of the corresponding data. For example, "action" refers to an output shape of [14], indicating
              14-dimensional actions. Importantly, shapes doesnt include batch dimension or temporal dimension.
          input_normalization_modes: A dictionary with key representing the modality (e.g. "observation.state"),
              and the value specifies the normalization mode to apply. The two available modes are "mean_std"
              which subtracts the mean and divides by the standard deviation and "min_max" which rescale in a
              [-1, 1] range.
          output_normalization_modes: Similar dictionary as `normalize_input_modes`, but to unnormalize to the
              original scale. Note that this is also used for normalizing the training targets.
          vision_backbone: Name of the torchvision resnet backbone to use for encoding images.
          crop_shape: (H, W) shape to crop images to as a preprocessing step for the vision backbone. Must fit
              within the image size. If None, no cropping is done.
          crop_is_random: Whether the crop should be random at training time (it's always a center crop in eval
              mode).
          pretrained_backbone_weights: Pretrained weights from torchvision to initalize the backbone.
              `None` means no pretrained weights.
          use_group_norm: Whether to replace batch normalization with group normalization in the backbone.
              The group sizes are set to be about 16 (to be precise, feature_dim // 16).
          spatial_softmax_num_keypoints: Number of keypoints for SpatialSoftmax.
          n_vqvae_training_steps: Number of optimization steps for training Residual VQ.
          vqvae_n_embed: Number of embedding vectors in the RVQ dictionary (each layer).
          vqvae_embedding_dim: Dimension of each embedding vector in the RVQ dictionary.
          vqvae_enc_hidden_dim: Size of hidden dimensions of Encoder / Decoder part of Residaul VQ-VAE
          gpt_block_size: Max block size of minGPT (should be larger than the number of input tokens)
          gpt_input_dim: Size of output input of GPT. This is also used as the dimension of observation features.
          gpt_output_dim: Size of output dimension of GPT. This is also used as a input dimension of offset / bin prediction headers.
          gpt_n_layer: Number of layers of GPT
          gpt_n_head: Number of headers of GPT
          gpt_hidden_dim: Size of hidden dimensions of GPT
          dropout: Dropout rate for GPT
          mlp_hidden_dim: Size of hidden dimensions of offset header / bin prediction headers parts of VQ-BeT
          offset_loss_weight:  A constant that is multiplied to the offset loss
          primary_code_loss_weight: A constant that is multiplied to the primary code prediction loss
          secondary_code_loss_weight: A constant that is multiplied to the secondary code prediction loss
          bet_softmax_temperature: Sampling temperature of code for rollout with VQ-BeT
          sequentially_select: Whether select code of primary / secondary as sequentially (pick primary code,
              and then select secodnary code), or at the same time.
      

  --policy.n_obs_steps int
  --policy.normalization_mapping Dict
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.n_action_pred_token int
  --policy.action_chunk_size int
  --policy.vision_backbone str
  --policy.crop_shape [int int]
  --policy.crop_is_random bool
  --policy.pretrained_backbone_weights [str]
  --policy.use_group_norm bool
  --policy.spatial_softmax_num_keypoints int
  --policy.n_vqvae_training_steps int
  --policy.vqvae_n_embed int
  --policy.vqvae_embedding_dim int
  --policy.vqvae_enc_hidden_dim int
  --policy.gpt_block_size int
  --policy.gpt_input_dim int
  --policy.gpt_output_dim int
  --policy.gpt_n_layer int
  --policy.gpt_n_head int
  --policy.gpt_hidden_dim int
  --policy.dropout float
  --policy.mlp_hidden_dim int
  --policy.offset_loss_weight float
  --policy.primary_code_loss_weight float
  --policy.secondary_code_loss_weight float
  --policy.bet_softmax_temperature float
  --policy.sequentially_select bool
  --policy.optimizer_lr float
                        Training presets (default: 0.0001)
  --policy.optimizer_betas Any
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.optimizer_vqvae_lr float
  --policy.optimizer_vqvae_weight_decay float
  --policy.scheduler_warmup_steps int

OfflineConfig ['offline']:

  --offline.steps int   

OnlineConfig ['online']:
  
      The online training loop looks something like:
  
      ```python
      for i in range(steps):
          do_online_rollout_and_update_online_buffer()
          for j in range(steps_between_rollouts):
              batch = next(dataloader_with_offline_and_online_data)
              loss = policy(batch)
              loss.backward()
              optimizer.step()
      ```
  
      Note that the online training loop adopts most of the options from the offline loop unless specified
      otherwise.
      

  --online.steps int    
  --online.rollout_n_episodes int
                        How many episodes to collect at once when we reach the
                        online rollout part of the training loop. (default: 1)
  --online.rollout_batch_size int
                        The number of environments to use in the
                        gym.vector.VectorEnv. This ends up also being the
                        batch size for the policy. Ideally you should set this
                        to by an even divisor of rollout_n_episodes. (default:
                        1)
  --online.steps_between_rollouts [int]
                        How many optimization steps (forward, backward,
                        optimizer step) to do between running rollouts.
                        (default: None)
  --online.sampling_ratio float
                        The proportion of online samples (vs offline samples)
                        to include in the online training batches. (default:
                        0.5)
  --online.env_seed [int]
                        First seed to use for the online rollout environment.
                        Seeds for subsequent rollouts are incremented by 1.
                        (default: None)
  --online.buffer_capacity [int]
                        Sets the maximum number of frames that are stored in
                        the online buffer for online training. The buffer is
                        FIFO. (default: None)
  --online.buffer_seed_size int
                        The minimum number of frames to have in the online
                        buffer before commencing online training. If
                        buffer_seed_size > rollout_n_episodes, the rollout
                        will be run multiple times until the seed size
                        condition is satisfied. (default: 0)
  --online.do_rollout_async bool
                        Whether to run the online rollouts asynchronously.
                        This means we can run the online training steps in
                        parallel with the rollouts. This might be advised if
                        your GPU has the bandwidth to handle training + eval +
                        environment rendering simultaneously. (default: False)

Optional ['optimizer']:

OptimizerConfig ['optimizer']:

  --optimizer.type {adam,adamw,sgd}
                        Which type of OptimizerConfig ['optimizer'] to use
                        (default: None)

AdamConfig ['optimizer']:

  --optimizer.lr float  
  --optimizer.weight_decay float
  --optimizer.grad_clip_norm float
  --optimizer.betas float float
  --optimizer.eps float

AdamWConfig ['optimizer']:

  --optimizer.lr float  
  --optimizer.weight_decay float
  --optimizer.grad_clip_norm float
  --optimizer.betas float float
  --optimizer.eps float

SGDConfig ['optimizer']:

  --optimizer.lr float  
  --optimizer.weight_decay float
  --optimizer.grad_clip_norm float
  --optimizer.momentum float
  --optimizer.dampening float
  --optimizer.nesterov bool

Optional ['scheduler']:

LRSchedulerConfig ['scheduler']:

  --scheduler.type {diffuser,vqbet,cosine_decay_with_warmup}
                        Which type of LRSchedulerConfig ['scheduler'] to use
                        (default: None)

DiffuserSchedulerConfig ['scheduler']:

  --scheduler.num_warmup_steps [int]
  --scheduler.name str  

VQBeTSchedulerConfig ['scheduler']:

  --scheduler.num_warmup_steps int
  --scheduler.num_vqvae_training_steps int
  --scheduler.num_cycles float

CosineDecayWithWarmupSchedulerConfig ['scheduler']:
  Used by Physical Intelligence to train Pi0

  --scheduler.num_warmup_steps int
  --scheduler.num_decay_steps int
  --scheduler.peak_lr float
  --scheduler.decay_lr float

EvalConfig ['eval']:

  --eval.n_episodes int
  --eval.batch_size int
                        `batch_size` specifies the number of environments to
                        use in a gym.vector.VectorEnv. (default: 50)
  --eval.use_async_envs bool
                        `use_async_envs` specifies whether to use asynchronous
                        environments (multiprocessing). (default: False)

WandBConfig ['wandb']:

  --wandb.enable bool   
  --wandb.disable_artifact bool
                        Set to true to disable saving an artifact despite
                        training.save_checkpoint=True (default: False)
  --wandb.project str   
  --wandb.entity [str]
  --wandb.notes [str]
